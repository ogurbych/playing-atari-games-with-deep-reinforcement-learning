{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c143e8-62e9-4fec-92e1-aafe198727f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Lambda, Add, Conv2D, Flatten\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import backend as K\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc5c7ec-5c37-4936-a685-12b65b88c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OurModel(input_shape, action_space, lr):\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    #X = Conv2D(32, 8, strides=(4, 4),padding=\"valid\", activation=\"elu\", data_format=\"channels_first\", input_shape=input_shape)(X_input)\n",
    "    #X = Conv2D(64, 4, strides=(2, 2),padding=\"valid\", activation=\"elu\", data_format=\"channels_first\")(X)\n",
    "    #X = Conv2D(64, 3, strides=(1, 1),padding=\"valid\", activation=\"elu\", data_format=\"channels_first\")(X)\n",
    "    X = Flatten(input_shape=input_shape)(X_input)\n",
    "\n",
    "    X = Dense(512, activation=\"elu\", kernel_initializer='he_uniform')(X)\n",
    "    #X = Dense(256, activation=\"elu\", kernel_initializer='he_uniform')(X)\n",
    "    #X = Dense(64, activation=\"elu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    action = Dense(action_space, activation=\"softmax\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    Actor = Model(inputs = X_input, outputs = action)\n",
    "    Actor.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=lr))\n",
    "\n",
    "    return Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8bcf7-e0a4-4491-a979-3eecd67b1a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGAgent:\n",
    "    # Policy Gradient Main Optimization Algorithm\n",
    "    def __init__(self, env_name):\n",
    "        # Initialization\n",
    "        # Environment and PG parameters\n",
    "        self.env_name = env_name       \n",
    "        self.env = gym.make(env_name)\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.EPISODES, self.max_average = 10000, -21.0 # specific for pong\n",
    "        self.lr = 0.000025\n",
    "\n",
    "        self.ROWS = 80\n",
    "        self.COLS = 80\n",
    "        self.REM_STEP = 4\n",
    "\n",
    "        # Instantiate games and plot memory\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "        self.scores, self.episodes, self.average = [], [], []\n",
    "\n",
    "        self.Save_Path = 'Models'\n",
    "        self.state_size = (self.REM_STEP, self.ROWS, self.COLS)\n",
    "        self.image_memory = np.zeros(self.state_size)\n",
    "        \n",
    "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
    "        self.path = '{}_PG_{}'.format(self.env_name, self.lr)\n",
    "        self.Model_name = os.path.join(self.Save_Path, self.path)\n",
    "\n",
    "        # Create Actor network model\n",
    "        self.Actor = OurModel(input_shape=self.state_size, action_space = self.action_size, lr=self.lr)\n",
    "\n",
    "    def remember(self, state, action, reward):\n",
    "        # store episode actions to memory\n",
    "        self.states.append(state)\n",
    "        action_onehot = np.zeros([self.action_size])\n",
    "        action_onehot[action] = 1\n",
    "        self.actions.append(action_onehot)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def act(self, state):\n",
    "        # Use the network to predict the next action to take, using the model\n",
    "        prediction = self.Actor.predict(state)[0]\n",
    "        action = np.random.choice(self.action_size, p=prediction)\n",
    "        return action\n",
    "\n",
    "    def discount_rewards(self, reward):\n",
    "        # Compute the gamma-discounted rewards over an episode\n",
    "        gamma = 0.99    # discount rate\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(reward)\n",
    "        for i in reversed(range(0,len(reward))):\n",
    "            if reward[i] != 0: # reset the sum, since this was a game boundary (pong specific!)\n",
    "                running_add = 0\n",
    "            running_add = running_add * gamma + reward[i]\n",
    "            discounted_r[i] = running_add\n",
    "\n",
    "        discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "        discounted_r /= np.std(discounted_r) # divide by standard deviation\n",
    "        return discounted_r\n",
    "\n",
    "    def replay(self):\n",
    "        # reshape memory to appropriate shape for training\n",
    "        states = np.vstack(self.states)\n",
    "        actions = np.vstack(self.actions)\n",
    "\n",
    "        # Compute discounted rewards\n",
    "        discounted_r = self.discount_rewards(self.rewards)\n",
    "\n",
    "        # training PG network\n",
    "        self.Actor.fit(states, actions, sample_weight=discounted_r, epochs=1, verbose=0)\n",
    "        # reset training memory\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "    \n",
    "    def load(self, Actor_name):\n",
    "        self.Actor = load_model(Actor_name, compile=False)\n",
    "\n",
    "    def save(self):\n",
    "        self.Actor.save(self.Model_name + '.h5')\n",
    "\n",
    "    pylab.figure(figsize=(18, 9))\n",
    "    def PlotModel(self, score, episode):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "        if str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            pylab.plot(self.episodes, self.scores, 'b')\n",
    "            pylab.plot(self.episodes, self.average, 'r')\n",
    "            pylab.ylabel('Score', fontsize=18)\n",
    "            pylab.xlabel('Steps', fontsize=18)\n",
    "            try:\n",
    "                pylab.savefig(self.path+\".png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "        return self.average[-1]\n",
    "\n",
    "    def imshow(self, image, rem_step=0):\n",
    "        cv2.imshow(self.Model_name+str(rem_step), image[rem_step,...])\n",
    "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "            return\n",
    "\n",
    "    def GetImage(self, frame):\n",
    "        # croping frame to 80x80 size\n",
    "        frame_cropped = frame[35:195:2, ::2,:]\n",
    "        if frame_cropped.shape[0] != self.COLS or frame_cropped.shape[1] != self.ROWS:\n",
    "            # OpenCV resize function \n",
    "            frame_cropped = cv2.resize(frame, (self.COLS, self.ROWS), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # converting to RGB (numpy way)\n",
    "        frame_rgb = 0.299*frame_cropped[:,:,0] + 0.587*frame_cropped[:,:,1] + 0.114*frame_cropped[:,:,2]\n",
    "\n",
    "        # convert everything to black and white (agent will train faster)\n",
    "        frame_rgb[frame_rgb < 100] = 0\n",
    "        frame_rgb[frame_rgb >= 100] = 255\n",
    "        # converting to RGB (OpenCV way)\n",
    "        #frame_rgb = cv2.cvtColor(frame_cropped, cv2.COLOR_RGB2GRAY)     \n",
    "\n",
    "        # dividing by 255 we expresses value to 0-1 representation\n",
    "        new_frame = np.array(frame_rgb).astype(np.float32) / 255.0\n",
    "\n",
    "        # push our data by 1 frame, similar as deq() function work\n",
    "        self.image_memory = np.roll(self.image_memory, 1, axis = 0)\n",
    "\n",
    "        # inserting new frame to free space\n",
    "        self.image_memory[0,:,:] = new_frame\n",
    "\n",
    "        # show image frame   \n",
    "        #self.imshow(self.image_memory,0)\n",
    "        #self.imshow(self.image_memory,1)\n",
    "        #self.imshow(self.image_memory,2)\n",
    "        #self.imshow(self.image_memory,3)\n",
    "        return np.expand_dims(self.image_memory, axis=0)\n",
    "\n",
    "    def reset(self):\n",
    "        frame = self.env.reset()\n",
    "        for i in range(self.REM_STEP):\n",
    "            state = self.GetImage(frame)\n",
    "        return state\n",
    "\n",
    "    def step(self,action):\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        next_state = self.GetImage(next_state)\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def run(self):\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.reset()\n",
    "            done, score, SAVING = False, 0, ''\n",
    "            while not done:\n",
    "                #self.env.render()\n",
    "                # Actor picks an action\n",
    "                action = self.act(state)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                next_state, reward, done, _ = self.step(action)\n",
    "                # Memorize (state, action, reward) for training\n",
    "                self.remember(state, action, reward)\n",
    "                # Update current state\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    average = self.PlotModel(score, e)\n",
    "                    # saving best models\n",
    "                    if average >= self.max_average:\n",
    "                        self.max_average = average\n",
    "                        self.save()\n",
    "                        SAVING = \"SAVING\"\n",
    "                    else:\n",
    "                        SAVING = \"\"\n",
    "                    print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(e, self.EPISODES, score, average, SAVING))\n",
    "\n",
    "                    self.replay()\n",
    "        \n",
    "        # close environemnt when finish training\n",
    "        self.env.close()\n",
    "\n",
    "    def test(self, Model_name):\n",
    "        self.load(Model_name)\n",
    "        for e in range(100):\n",
    "            state = self.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.Actor.predict(state))\n",
    "                state, reward, done, _ = self.step(action)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, score))\n",
    "                    break\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fc3d70-4ec6-426e-ab96-7dab06cc1a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #env_name = 'Pong-v0'\n",
    "    env_name = 'PongDeterministic-v4'\n",
    "    agent = PGAgent(env_name)\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786ebed9-0d04-4da3-baf9-8004ed881aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
